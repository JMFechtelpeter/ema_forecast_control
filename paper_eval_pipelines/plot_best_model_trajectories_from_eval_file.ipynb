{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine relevant evaluation files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grand combined evaluation file                                                                                                                                                                 "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import pandas as pd\n",
    "import eval_reallabor_utils\n",
    "import data_utils\n",
    "\n",
    "MRT = [2, 3]\n",
    "combined_eval_file = []\n",
    "\n",
    "for mrt in MRT:\n",
    "\n",
    "    results_dirs = [\n",
    "        f'v3_MRT{mrt}_SimpleModels_every_day/00_summary_7stepsahead_interv',\n",
    "        f'v3_MRT{mrt}_Kalman_every_day/00_summary_7stepsahead_interv',\n",
    "        f'v3_MRT{mrt}_every_day/00_summary_7stepsahead_interv',\n",
    "        f'v3_MRT{mrt}_Transformer_every_day/00_summary_7stepsahead_interv'\n",
    "        ]\n",
    "\n",
    "    eval_files = []\n",
    "    for rd in results_dirs:\n",
    "        eval_files.append(data_utils.join_ordinal_bptt_path('results', rd, 'evaluation.csv'))\n",
    "        assert os.path.exists(eval_files[-1]), eval_files[-1]\n",
    "    eval_reallabor_utils.clear_line_and_print(f'Combining evaluation files')\n",
    "    mrt_combined_eval_file = eval_reallabor_utils.combine_evaluation_files(eval_files, save_path=None, specifier_name='latent_model')\n",
    "    mrt_combined_eval_file['MRT'] = mrt\n",
    "\n",
    "    use_days_from_file = data_utils.train_test_split_path(mrt, 'valid_first_alarms_no_con_smoothed.csv')\n",
    "    valid_days = pd.read_csv(use_days_from_file, index_col=0)\n",
    "    valid_days.columns = [int(c) for c in valid_days.columns]\n",
    "\n",
    "    eval_reallabor_utils.clear_line_and_print(f'Filtering out invalid days')\n",
    "    eval_only_valid_days = []\n",
    "    for p, group in mrt_combined_eval_file.groupby('participant'):\n",
    "        if p in valid_days.columns:\n",
    "            eval_only_valid_days.append(group.loc[group['test_day'].isin(valid_days[p])])\n",
    "    mrt_combined_eval_file = pd.concat(eval_only_valid_days, axis=0, ignore_index=True)\n",
    "    \n",
    "    mrt_combined_eval_file['prediction'] = mrt_combined_eval_file['prediction'].clip(0, 8)\n",
    "\n",
    "    combined_eval_file.append(mrt_combined_eval_file)\n",
    "\n",
    "eval_reallabor_utils.clear_line_and_print(f'Creating grand combined evaluation file')\n",
    "combined_eval_file = pd.concat(combined_eval_file, axis=0, ignore_index=False)\n",
    "if not os.path.exists(data_utils.join_ordinal_bptt_path('results/_paper/combined_every_day_ensemble.csv')):\n",
    "    os.makedirs(data_utils.join_ordinal_bptt_path('results/_paper'), exist_ok=True)\n",
    "    combined_eval_file.to_csv(data_utils.join_ordinal_bptt_path('results/_paper/combined_every_day_ensemble.csv'))\n",
    "\n",
    "if 'train_until' in combined_eval_file.columns:\n",
    "    combined_eval_file.loc[combined_eval_file['latent_model']=='hierarchized clipped-shallow-PLRNN', 'latent_model'] = 'hierarchized-clipped-shallow-PLRNN'\n",
    "    combined_eval_file.loc[combined_eval_file['train_until'].isna(), 'train_until'] = combined_eval_file.loc[combined_eval_file['train_until'].isna(), 'train_on_data_until_timestep']\n",
    "\n",
    "combined_eval_file = eval_reallabor_utils.include_exclude_hypers(combined_eval_file, {}, {'steps':7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_reallabor import reallabor_metrics\n",
    "m_manager = reallabor_metrics.MetricsManager(combined_eval_file, ['MRT', 'latent_model', 'participant'], use_gt_for_predicted_difference=False, include_r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = m_manager.mae().unstack('latent_model')\n",
    "difference_to_plrnn = mae - mae[['clipped-shallow-PLRNN']].to_numpy()  # positive = PLRNN wins, negative = other wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plrnn_var_best = difference_to_plrnn['VAR1'].sort_values(ascending=False)#.head(5)\n",
    "plrnn_kalman_best = difference_to_plrnn['KalmanFilter'].sort_values(ascending=False)#.head(5)\n",
    "plrnn_var_kalman_best = difference_to_plrnn[['VAR1', 'KalmanFilter']].sum(axis=1).sort_values(ascending=False)#.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW = False\n",
    "REALTIME = False\n",
    "SORTBY = plrnn_var_kalman_best\n",
    "\n",
    "os.makedirs(data_utils.join_ordinal_bptt_path('results/_paper/predictions'), exist_ok=True)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from math import ceil, floor\n",
    "from plotting_styles import PaperStyle, colors\n",
    "from plotting_utils import adjust_lim\n",
    "\n",
    "model_labels = {\n",
    "                    # 'MovingAverage(1)': 'Last Step', 'MeanPredictor': 'Global Mean', 'InputsRegression': 'Linear Regression', \n",
    "                    'VAR1': 'VAR(1)', \n",
    "                    'KalmanFilter': 'Kalman Filter', \n",
    "                    'clipped-shallow-PLRNN': 'PLRNN', \n",
    "                    # 'hierarchized-clipped-shallow-PLRNN': 'H-PLRNN',\n",
    "                    'Transformer': 'Transformer'\n",
    "                    }\n",
    "\n",
    "mae_errorbars = m_manager.mae(func='sem').unstack('latent_model')\n",
    "\n",
    "m_manager_stepwise = reallabor_metrics.MetricsManager(combined_eval_file, ['MRT', 'latent_model', 'participant', 'steps'], use_gt_for_predicted_difference=False, include_r2=False)\n",
    "\n",
    "with PaperStyle():\n",
    "\n",
    "    for i in range(len(SORTBY)):\n",
    "        participant = SORTBY.index.get_level_values('participant')[i]\n",
    "        MRT = SORTBY.index.get_level_values('MRT')[i]\n",
    "        eval_xs = m_manager_stepwise.raw_metrics.xs(SORTBY.index[i], level=SORTBY.index.names, drop_level=False).sort_values(['train_on_data_until_timestep', 'steps']).reset_index()\n",
    "        eval_xs.loc[eval_xs['steps']==0, 'prediction'] = eval_xs.loc[eval_xs['steps']==0, 'ground_truth']\n",
    "\n",
    "        eval_xs['timesteps'] = eval_xs[['steps', 'train_on_data_until_timestep']].sum(axis=1).to_numpy()\n",
    "        first_alarms = eval_xs.loc[eval_xs['steps'] == 0, 'timesteps'].unique()\n",
    "\n",
    "        participant_data = pd.read_csv(data_utils.single_subject_path(MRT, 'processed_csv_no_con_smoothed_causal', participant))\n",
    "        day_nr = participant_data.loc[participant_data['Timesteps'].isin(first_alarms), 'DayNr']\n",
    "\n",
    "        if not REALTIME:\n",
    "            eval_xs['timesteps'] = eval_xs['timesteps'].astype(str)  \n",
    "            first_alarms = first_alarms.astype(str)\n",
    "\n",
    "        if len(first_alarms) != len(day_nr):\n",
    "            1\n",
    "\n",
    "        # gt = eval_xs.loc[eval_xs['latent_model']=='clipped-shallow-PLRNN', 'ground_truth'] \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 2), width_ratios=(5,1))\n",
    "        axes[0].plot(eval_xs['timesteps'], eval_xs['ground_truth'], label='ground truth', linestyle='', marker='.')\n",
    "        for m, model in enumerate(model_labels.keys()):\n",
    "            for n, ts in enumerate(eval_xs['train_on_data_until_timestep'].unique()):\n",
    "                mask = (eval_xs['latent_model'] == model) & (eval_xs['train_on_data_until_timestep'] == ts)\n",
    "                axes[0].plot(eval_xs.loc[mask, 'timesteps'], eval_xs.loc[mask, 'prediction'], label=model_labels[model], color=colors.model_colors[model])\n",
    "            axes[1].errorbar(m, mae.xs(SORTBY.index[i], level=SORTBY.index.names)[model], \n",
    "                            yerr=mae_errorbars.xs(SORTBY.index[i], level=SORTBY.index.names)[model],\n",
    "                            linestyle='', marker='o', color=colors.model_colors[model])\n",
    "        # axes[0].legend()\n",
    "        axes[0].set(xlabel='day', ylabel='mean EMA score', title=f'Participant {participant}', xticks=first_alarms, xticklabels=day_nr)\n",
    "        axes[1].set_xticks(range(len(model_labels)), labels=model_labels.values(), rotation=90)\n",
    "\n",
    "\n",
    "        ylim = list(axes[0].get_ylim())\n",
    "        ylim[0] = floor(ylim[0])\n",
    "        ylim[1] = ceil(ylim[1])\n",
    "        for fa in first_alarms:\n",
    "            axes[0].plot((fa, fa), ylim, color='white', zorder=0)\n",
    "\n",
    "        adjust_lim(axes[0], axis='y', upper=0.05, lower=0.05)\n",
    "        adjust_lim(axes[1], axis='x', upper=0.05, lower=0.05)\n",
    "        adjust_lim(axes[1], axis='y', upper=0.05, lower=0.05)\n",
    "        if SAVE:\n",
    "            plt.savefig(data_utils.join_ordinal_bptt_path('results/_paper/predictions', f'participant_{participant}.svg'))\n",
    "        if not SHOW:\n",
    "            plt.close(fig)\n",
    "if SHOW:\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reallaborai4u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
